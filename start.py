import PyPDF2
import spacy
import nltk
import re
from summa import keywords

from random import seed
from random import randint

import operator

from collections import defaultdict, OrderedDict

seed(1)
output_pdf = "output.pdf"
pre_output_file = "pre_output.txt"
pdf_flag = 1
karteikarten = []
nlp = spacy.load("en_core_web_sm")
#nlp = spacy.load("de_core_news_md")


def get_Keywords(text):
    keywords_textrank = keywords.keywords(text)
    doc = nlp(keywords_textrank)
    keywords_list = []
    for token in doc:
        if token.pos_ == "PROPN" or token.pos_ == "NOUN":
            keywords_list.append(token.text)

    return keywords_list



def getTextFromPDF(file_name):
    pdfFileObj = open('data/'+file_name,'rb')     #'rb' for read binary mode
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)
    numPages = pdfReader.numPages
    text = ""
    #text_dict = []
    text_folie= {}

    for i in range(0,numPages):
        pageObj = pdfReader.getPage(i)
        text = text + " "+ pageObj.extractText()
        text_folie[i] = pageObj.extractText()

        #text_dict[i] = pageObj.extractText()

    return (text, pdfFileObj, text_folie)


def getTextFromFile(file_name):
    with open('data/'+file_name+'', 'r') as file:
        data = file.read()
    return data

def _word_distribution(sentence_processed):
    """Compute word probabilistic distribution
    """
    word_distr = defaultdict(int)
    word_count = 0.0

    for k in sentence_processed:
        for word_l in sentence_processed[k]:
            word = word_l.text
            word_distr[word] += 1
            word_count += 1

    for word in word_distr:
        word_distr[word] = word_distr[word] / word_count

    return word_distr

def _sentence_weight(word_distribution, sentence_processed):
    """Compute weight with respect to sentences
    Args:
            word_distribution: probabilistic distribution of terms in document
            sentence_processed: dict of processed sentences generated by pre_processing
    Return:
            sentence_weight: dict of weight of each sentence
    """
    sentence_weight = {}

    for sentence_id in sentence_processed:
        for word_l in sentence_processed[sentence_id]:
            word = word_l.text
            if word_distribution[word] and sentence_id in sentence_weight:
                sentence_weight[sentence_id] += word_distribution[word]
            else:
                sentence_weight[sentence_id] = word_distribution[word]

        sentence_weight[sentence_id] = sentence_weight[
            sentence_id] / float(len(sentence_processed[sentence_id]))

    sentence_weight = sorted(sentence_weight.items(), key=operator.itemgetter(1), reverse=True)
    return sentence_weight

def output(output_text, output_file):
    with open(output_file, 'w') as file:
        data = file.write(output_text)

def pre_output(output_text):
    output = ""
    i = 0
    for key in output_text:
        output+=str(key)+output_text[key].text+"\n"

    with open(pre_output_file, 'w') as file:
        data = file.write(output)

def pre_processing2(source_text):

    (data, pdfFileObj, data_folie) = getTextFromPDF(source_text)

    sentence_folien_map = {}
    folien_num = 1
    i = 0
    sentence_dic = {}
    raw_text=""

    for key in data_folie:
        data = data_folie[key]
        #todo highlight text in outputfile
        data = data.lower()
        data = data.replace("\n"," ",1)

        raw_text += data+" "

        data = re.sub(' +', ' ', data)
        doc = nlp(data)


        #sentence seperation
        sentence_spans = list(doc.sents)

        for sents in sentence_spans:
            sents_str = sents.text
            sents_str = sents_str.rstrip("\r\n")
            sents_str = sents_str.replace("\n", " ")

            sents_str = re.sub(' +', ' ', sents_str)
            tokenlist = nlp(sents_str)
            count = 0
            hasVerb = False
            hasNomen = False
            for token in tokenlist:
                if token.pos_ == "VERB":
                    hasVerb = True
                #if token.pos_ == "NOUN" or token.pos_ == "PROPN":
                #    hasNomen = True
                count+=1

            if count>5 and hasVerb:
                sentence_dic[i] = tokenlist
                sentence_folien_map[i] = folien_num
                i+=1

        folien_num+=1

    return sentence_dic, raw_text, sentence_folien_map



def pre_processing(source_text):
    #nlp = spacy.load("de_core_news_sm")

    if pdf_flag:
        (data, pdfFileObj, data_folie) = getTextFromPDF(source_text)
    else:
        data = getTextFromFile(source_text)

    #todo highlight text in outputfile
    data = data.lower()

    #special case
    if source_text == "paper.pdf":
        data = data.replace("grundlagen der betriebswirtschaftslehre ii","")

    data = data.replace("\n"," ",1)

    raw_text = data

    data = re.sub(' +', ' ', data)

    doc = nlp(data)

    #ACHTUNG: hier unten ein fehler bei formatierung!?

    #sentence seperation
    sentence_spans = list(doc.sents)
    for sent in doc.sents:
        #print(sent.text)
        #tokenization
        i = 0
        sentence_dic = {}
    for sents in sentence_spans:
        sents_str = sents.text
        sents_str = sents_str.rstrip("\r\n")
        sents_str = sents_str.replace("\n", " ")

        #sents_str = re.sub(' +', ' ', sents_str)

        tokenlist = nlp(sents_str)
        count = 0
        hasVerb = False
        for token in tokenlist:
            if token.pos_ == "VERB":
                hasVerb = True
            count+=1

        if count>5 and hasVerb:
            sentence_dic[i] = tokenlist
            i+=1

    return sentence_dic, raw_text


def generate_question(sentence, word_distribution, keywords):
    doc=nlp(sentence)
    candidates=[]

    for token in doc:
        if token.pos_ == "NOUN" or token.pos_ == "PROPN":
            candidates.append(token.text)

    #todo if kein nomen dabei
    if len(candidates) == 0:
            #print("no candidates found")
            for token in doc:
                candidates.append(token.text)


    answer=""
    highest_rate=1 #oder lowest? 0 oder 1
    other_answers=[]
    for word in candidates:
        cur_rate = word_distribution[word]
        if cur_rate<highest_rate: #change hier
            answer=word
            highest_rate=cur_rate

    if answer=="":
        print("error1")

    keywords_length = len(keywords)
    while True:
        value = randint(0, keywords_length-1)
        cur_word = keywords[value]

        if cur_word != answer:
            other_answers.append(cur_word)
        if len(other_answers) ==3:
            break

    return (answer, other_answers)






sentence_dic, rawtext, sentence_to_folie_map = pre_processing2("paper.pdf")
pre_output(sentence_dic)

# for key in sentence_dic:
#     print(str(key)+": "+ sentence_dic[key].text)

word_distribution = _word_distribution(sentence_dic)

sentence_weight = _sentence_weight(word_distribution, sentence_dic)
#print(sentence_weight)


output_question = ""
output_text = ""
keywords = get_Keywords(rawtext)
output(str(keywords), "output_keywords.txt")
error = 0


if len(sentence_weight) < 30:
    error = 1
    print("zu wenig Text")


if error == 0:
    for i in range(0, 10):
        sent_num = sentence_weight[i][0]
        sentence = sentence_dic[sent_num].text
        folie = sentence_to_folie_map[sent_num]
        output_text += str(i)+": "+sentence + " folie: "+str(folie) +"\n"
        answer, otheranswers = generate_question(sentence, word_distribution, keywords)
        sentence_blank = sentence.replace(answer,"______________")
        output_question += str(i)+": "+sentence_blank + "\n"
        output_question+="(a) "+answer+"\n"
        output_question+="(b) "+otheranswers[0]+"\n"
        output_question+="(c) "+otheranswers[1]+"\n"
        output_question+="(d) "+otheranswers[2]+"\n"

    output(output_question, "output_questions.txt")
    output(output_text, "output_text.txt")




#todo include chunks
#chunks
'''
for chunk in doc.noun_chunks:
    print("1 "+chunk.text +" 2 "+ chunk.root.text +" 4 "+ chunk.root.head.text)
    print(chunk.text)
'''





#sentence_spans = list(doc.sents)
#print(displacy.serve(sentence_spans, style="dep"))
